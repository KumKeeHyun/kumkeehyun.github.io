---
published: false

title: "빅데이터프로그래밍 프로젝트 1편"
subtitle: "프로젝트 구상 및 설계하기"
permalink: posts/bdp-project/intro
classes: wide

categories:
  - bdp project
tags:
  - data analysis
  - kafka
  - hive
  - hbase
  - airflow
  - twitter api
last_modified_at: 2023-03-18T00:00:00-00:00
---

# TOC
<!--ts-->
- [TOC](#toc)
- [서론](#서론)
- [주제 선정](#주제-선정)
- [설계](#설계)
  - [데이터 구하기](#데이터-구하기)
  - [ETL 파이프라인 구상하기](#etl-파이프라인-구상하기)
    - [Data Warehouse](#data-warehouse)
    - [Data Processing](#data-processing)
  - [데이터 분석하기](#데이터-분석하기)
<!--te-->

# 서론

'23 봄학기에 수강하고 있는 빅데이터프로그래밍 강의가 프로젝트 강의~~(PDL, Project-Driven-Lecture...ㅋㅋ)~~라서 데이터를 수집 및 분석하는 프로젝트를 진행해야 한다. 평가 목록에 데이터의 크기는 상관 없다고 하셨지만 그래도 **빅데이터**프로그래밍인 만큼 ETL 파이프라인을 구축해보는 방향으로 구상했다. 

기술이 목적보다 우선시되면 안된다고 생각하지만, 사실 이력서에 쓸만한 기술을 사용해보자는 목표로 이 강의를 신청했기 때문에 죄책감은 느끼지 않기로 했다.

# 주제 선정

첫 수업 때 교수님이 N사에 계실 때 진행하셨던 미세먼지 기사 분석을 프로젝트 예시로 소개해주셨다. 미세먼지와 관련된 7000개 정도의 기사를 work2vec을 이용하여 가공한 뒤에 클러스터링으로 미세먼지와 관련된 키워드들을 뽑는 프로젝트였다.

사실 지금까지 데이터 처리에만 관심이 있었지, 분석에 대해선 진지하게 생각해본 적이 없었기 때문에 막막했다. 그러던 중 교수님이 '일단 나 자신이 재미있어야 한다'라고 하셔서, 요즘 핫한 `ChatGPT의 트렌드`에 대해 알아보는 것을 주제로 선정했다. 분석해볼 세부 주제는 다음과 같다.

- ChatGPT를 많이 사용하는 직종은 무엇일까?
- 사람들은 ChatGPT를 어떤 목적으로 사용하고 있을까?

# 설계

## 데이터 구하기

우선 사람들이 ChatGPT에 대한 의견을 써놓는 커뮤니티에서 데이터를 얻어와야 한다. 데이터에는 `ChatGPT에 관련된 글`과 `해당 글을 쓴 사람 정보`가 필요할 것이라 판단했고 이러한 데이터르 얻을 수 있는 open api를 찾아보았다.

사실 주제를 정할 때부터 Twitter API를 사용해서 데이터를 얻어오는 구상을 했었다. 특정 키워드를 필터링해서 트윗을 읽어올 수 있고, 해당 트윗의 유저 정보도 얻어올 수 있기 때문에 안성맞춤이었다. 또, kafka-connect-twitter를 이용하면 분석할 데이터를 준비하는데 큰 시간을 소비하지 않을 것이라 판단했다. 

## ETL 파이프라인 구상하기

우선 전체적인 흐름을 다음과 같이 구상했었다. `twitter api` 및 `kafka connect`를 활용해서 Kafka에 데이터를 수집해놓는 것 까지는 확정한 상태였고, 이후에 데이터를 어떻게 가공할지, 어떤 곳에 저장하고 분석할지를 결정해야 했다.

```
+---------+   twitter api    +-------+  some processing   +----------------+  csv or sql   +----------+
| Twitter | ---------------> | Kafka | -----------------> | Data Warehouse | ------------> | analysis |
+---------+  kafka connect   +-------+                    +----------------+               +----------+
```

우선 현재 상황에 따른 요구 사항을 정리해보았다.

- 데이터 분석을 어떻게 진행할지 정하지 못했다
- 가공한 데이터를 모아서 한번에 분석하는 것까지는 결정했다
  - 데이터를 분석은 실시간으로 이뤄질 필요가 없다
  - 데이터 웨어하우스는 interactive query를 지원하지 않아도 상관없다
  - 최소한 데이터를 csv 형태로 뽑아낼 수 있어야 한다
- 데이터 소스가 Kafka로 고정이다
- 데이터 분석을 어떻게 할 지 모르기 때문에 데이터 가공 형태가 많이 바뀔 수 있다
  - 데이터 가공은 Kafka와 통합하기 쉬워야 한다
  - 실시간 처리, 배치 처리 상관 없이, 목표로 하는 형태로 가공만 할 수 있으면 상관 없다
  - 다루기 쉽고 개발이 빠른 플랫폼이면 좋다

다음으로 Data Warehouse로 어떤 것을 선택할지 고민했다.

### Data Warehouse

Data Engineer가 되고 싶으면서 아직 하둡을 다뤄본 적이 없다는게 마음에 걸려서 하둡 생태계에서 선택하기로 했다. 하지만 HDFS을 기반으로 사용할 수 있는 HBase, Hive, Impala 같은 다양한 옵션들을 알고는 있었지만 각각 어떤 이점이 있는지는 정확히 알지 못했다. 결국 ChatGPT에게 물어보고 구글 검색으로 사실확인을 해보는 식으로 결정했다.

<img width="725" alt="image" src="https://user-images.githubusercontent.com/44857109/226093478-156af67e-012e-4071-958b-667e2f72abc0.png">

먼저 HBase에 대해서 물어봤는데 많은 양의 반정형, 비정형 데이터를 저장할 수 있고, 큰 데이터를 실시간 쿼리할 수 있다고 설명해주었다. 추가로 좀더 구조화된 데이터에 대해서 복잡한 쿼리를 수행할 수 있는 Hive와 Pig를 추천받았다.

Hive는 선택 후보에 있었기 때문에 다음으로는 Hive의 이점에 대해 물어보았다.

<img width="732" alt="image" src="https://user-images.githubusercontent.com/44857109/226093870-7ccda2e1-e331-4771-9917-b0bf3d1d1c4c.png">

HDFS에 저장된 데이터를 SQL과 유사한 인터페이스로 쿼리할 수 있다는 것이 큰 장점이라 설명해준다. 또한 csv 형태로 데이터를 뽑을 수도 있다고 해서 합리적인 선택같았다.

다음으로는 Hive와 같이 SQL interface를 제공해주는 Impala에 대해 물어보았다.

<img width="725" alt="image" src="https://user-images.githubusercontent.com/44857109/226094265-7811eb7e-f5b5-4279-a8a7-8a32614bdb4c.png">

- Hive는 MapReduce를 사용하고(v2부터는 tez 사용가능) 큰 데이터셋에 대해 배치 처리하는 것에 최적화 되어있음
- Impala는 MPP(Massively Parallel Processing)를 사용하고 비교적 작은 데이터셋에 대해 실시간 쿼리를 목표로 개발되었음

Impala가 전통적인 SQL에 더 가까운 인터페이스를 지원해주지만, 딱히 지연시간이 적어야 하거나 실시간 쿼리가 필요하지 않기 때문에 Hive를 사용하기로 했다.

### Data Processing

Data Processing에서 기술 후보는 `Kakfa Streams`, `Flink`, `Spark`였다. 

source(kafka)와 sink(hive(hdfs))가 모두 정해졌기 때문에 두가지와 통합하기 쉽고, 개발/운영 비용이 적은 것을 기준으로 고려했다.

Kafka Streams는 전에 한번 사용해본 적이 있어서 친숙했고, 나머지 Flink와 Spark는 새롭게 공부해야 했던 기술이었다. 
이미 이번 학기 동안에 할 것이 많았기 때문에 이미 새로 Hive를 사용해보는 것도 부담스러웠다. 그래서 Flink와 Spark는 꺼려지는 느낌이 있었다.
또, Spark와 Flink는 별도 클러스터 운영이 필요하기 때문에 그냥 친숙한 Kafka Streams를 사용해서 개발하기로 결정했다.

Hive로의 저장은 kafka connect hdfs sink connector를 사용하기로 했다.

최종으로 구상한 ETL 파이프라인은 다음과 같다.

```
+---------+   twitter api    +-------+  kafka streams   +-------+                  +--------------+  csv or sql   +----------+
| Twitter | ---------------> | Kafka | ---------------> | Kafka | ---------------> | Hive on HDFS | ------------> | analysis |
+---------+  kafka connect   +-------+                  +-------+  kafka connect   +--------------+               +----------+
```

## 데이터 분석하기

아직 제대로된 계획은 없다. 따로 라벨링을 하지 않을 예정이라 클러스터링을 이용해야 하지 않을까 싶다. 1차 발표 후에 교수님의 조언을 토대로 구체화하고자 한다.